{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# nasty hack for Colab\n",
        "![ -n $COLAB_RELEASE_TAG ] && git clone -b madziejm-dev https://github.com/madziejm/0nmt.git\n",
        "![ -n $COLAB_RELEASE_TAG ] && pip install -r ./0nmt/requirements.txt\n",
        "try:\n",
        "  import google.colab\n",
        "  import sys\n",
        "  sys.path.insert(0, '/content/0nmt')\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import pytorch_lightning.callbacks as plc\n",
        "import torch\n",
        "from icecream import ic\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "from torchtext.vocab import FastText, vocab\n",
        "\n",
        "from zeronmt.models.datatypes import DimensionSpec, Language, Vectors\n",
        "from zeronmt.models.seq2seq import Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "url_base = \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/\"\n",
        "train_urls = (\"train.de.gz\", \"train.en.gz\")\n",
        "val_urls = (\"val.de.gz\", \"val.en.gz\")\n",
        "test_urls = (\"test_2016_flickr.de.gz\", \"test_2016_flickr.en.gz\")\n",
        "\n",
        "train_filepaths = [\n",
        "    extract_archive(download_from_url(url_base + url))[0] for url in train_urls\n",
        "]\n",
        "val_filepaths = [\n",
        "    extract_archive(download_from_url(url_base + url))[0] for url in val_urls\n",
        "]\n",
        "test_filepaths = [\n",
        "    extract_archive(download_from_url(url_base + url))[0] for url in test_urls\n",
        "]\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")  # keep it simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MAPPING_PATH = Path(\n",
        "#     \"/home/maciej/github/bachelor-thesis/project/vecs/le0n8xvt7l/best_mapping.pth\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # TODO\n",
        "# mapping = torch.load(MAPPING_PATH)\n",
        "\n",
        "# cs_vecs = MappedFastTextVectors(language=\"cs\", mapping=None)\n",
        "# pl_vecs = MappedFastTextVectors(language=\"pl\", mapping=mapping)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`src` means DE.  \n",
        "`tgt` means ENG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FastTextPretrainedAligned(FastText):\n",
        "    url_base = (\n",
        "        \"https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.{}.align.vec\"\n",
        "    )\n",
        "    # url_base = \"https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.{}.align.vec\"\n",
        "\n",
        "    def __init__(self, language: str, special_toks: List[str], **kwargs) -> None:\n",
        "        super().__init__(language, **kwargs)\n",
        "\n",
        "        # prepend specials tokens\n",
        "        self.itos[0:0] = special_toks\n",
        "\n",
        "        # hopefully it is not slow :)\n",
        "        self.stoi = {\n",
        "            **dict(zip(special_toks, range(len(special_toks)))),\n",
        "            **{word: i + len(special_toks) for i, word in enumerate(self.stoi)},\n",
        "        }\n",
        "\n",
        "        # the vectors for the special tokens here will not be used by the model\n",
        "        # we set them to zeros so indexing works flawlessly\n",
        "        vecs_special_toks = torch.zeros(len(special_toks), self.dim)\n",
        "        self.vectors = torch.cat((vecs_special_toks, self.vectors), dim=0)\n",
        "        assert len(self.vectors) == len(self.itos)\n",
        "        assert len(self.vectors) == len(self.stoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "VOCAB_SIZE = int(5e4)  # top 50K words only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "specials = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
        "\n",
        "tgt_vecs = FastTextPretrainedAligned(\n",
        "    language=\"en\", special_toks=specials, max_vectors=VOCAB_SIZE\n",
        ")\n",
        "src_vecs = FastTextPretrainedAligned(\n",
        "    language=\"de\", special_toks=specials, max_vectors=VOCAB_SIZE\n",
        ")\n",
        "\n",
        "tgt_vocab = vocab(tgt_vecs.stoi, min_freq=0)\n",
        "src_vocab = vocab(src_vecs.stoi, min_freq=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "src_vocab.set_default_index(src_vocab[\"<unk>\"])\n",
        "tgt_vocab.set_default_index(tgt_vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| src_vecs.stoi[\"<unk>\"]: 0\n",
            "ic| src_vecs.stoi[\"<pad>\"]: 1\n",
            "ic| src_vecs.stoi[\"<bos>\"]: 2\n",
            "ic| src_vecs.stoi[\"<eos>\"]: 3\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ic(src_vecs.stoi[\"<unk>\"])\n",
        "ic(src_vecs.stoi[\"<pad>\"])\n",
        "ic(src_vecs.stoi[\"<bos>\"])\n",
        "ic(src_vecs.stoi[\"<eos>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "# special tokens are prepended, so these indices are the same for both the languages\n",
        "PAD_IDX = src_vocab[\"<pad>\"]\n",
        "BOS_IDX = src_vocab[\"<bos>\"]\n",
        "EOS_IDX = src_vocab[\"<eos>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| PAD_IDX: 1\n",
            "ic| BOS_IDX: 2\n",
            "ic| EOS_IDX: 3\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ic(PAD_IDX)\n",
        "ic(BOS_IDX)\n",
        "ic(EOS_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO\n",
        "# INPUT_DIM = len(cs_vecs)\n",
        "# OUTPUT_DIM = len(pl_vecs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# INPUT_DIM = len(src_vecs)\n",
        "# OUTPUT_DIM = len(tgt_vecs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ic(INPUT_DIM)\n",
        "# ic(OUTPUT_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENC_HID_DIM = 64\n",
        "DEC_HID_DIM = 64\n",
        "ATTN_DIM = 8\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_process(filepaths):\n",
        "    raw_src_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "    raw_tgt_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "    data = []\n",
        "    for raw_de, raw_en in zip(raw_src_iter, raw_tgt_iter):\n",
        "        src_tensor_ = torch.tensor(\n",
        "            [src_vocab[token] for token in tokenizer(raw_de)], dtype=torch.long\n",
        "        )\n",
        "        tgt_tensor_ = torch.tensor(\n",
        "            [tgt_vocab[token] for token in tokenizer(raw_en)], dtype=torch.long\n",
        "        )\n",
        "        data.append((src_tensor_, tgt_tensor_))\n",
        "    return data\n",
        "\n",
        "\n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoder.special_toks_embedding.weight\n",
            "encoder.rnn.weight_ih_l0\n",
            "encoder.rnn.weight_hh_l0\n",
            "encoder.rnn.bias_ih_l0\n",
            "encoder.rnn.bias_hh_l0\n",
            "encoder.rnn.weight_ih_l0_reverse\n",
            "encoder.rnn.weight_hh_l0_reverse\n",
            "encoder.rnn.bias_ih_l0_reverse\n",
            "encoder.rnn.bias_hh_l0_reverse\n",
            "encoder.fc.weight\n",
            "encoder.fc.bias\n",
            "decoder.attention.attn.weight\n",
            "decoder.attention.attn.bias\n",
            "decoder.special_toks_embedding.weight\n",
            "decoder.rnn.weight_ih_l0\n",
            "decoder.rnn.weight_hh_l0\n",
            "decoder.rnn.bias_ih_l0\n",
            "decoder.rnn.bias_hh_l0\n",
            "decoder.output_to_src.weight\n",
            "decoder.output_to_src.bias\n",
            "decoder.output_to_tgt.weight\n",
            "decoder.output_to_tgt.bias\n"
          ]
        }
      ],
      "source": [
        "# enc = Encoder(\n",
        "#     INPUT_DIM, tgt_vecs, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT, PAD_IDX, len(specials)\n",
        "# )\n",
        "# attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
        "# dec = Decoder(\n",
        "#     OUTPUT_DIM,\n",
        "#     src_vecs,\n",
        "#     ENC_HID_DIM,\n",
        "#     DEC_HID_DIM,\n",
        "#     DEC_DROPOUT,\n",
        "#     attn,\n",
        "#     PAD_IDX,\n",
        "#     len(specials),\n",
        "# )\n",
        "\n",
        "# model = Seq2Seq(\n",
        "#     INPUT_DIM,\n",
        "#     PAD_IDX=PAD_IDX\n",
        "#     ).to(device)\n",
        "model = Seq2Seq(\n",
        "    DEC_DROPOUT,\n",
        "    ENC_DROPOUT,\n",
        "    DimensionSpec(\n",
        "        attention=ATTN_DIM,\n",
        "        dec_hid=DEC_HID_DIM,\n",
        "        enc_hid=ENC_HID_DIM,\n",
        "        nspecial_toks=len(specials),\n",
        "    ),\n",
        "    PAD_IDX,\n",
        "    Vectors(src_vecs, tgt_vecs),\n",
        ")\n",
        "# src_pretrained_embeddings=src_vecs,\n",
        "# tgt_pretrained_embeddings=tgt_vecs,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (special_toks_embedding): Embedding(4, 300, padding_idx=1)\n",
              "    (rnn): GRU(300, 64, bidirectional=True)\n",
              "    (fc): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=192, out_features=8, bias=True)\n",
              "    )\n",
              "    (special_toks_embedding): Embedding(4, 300, padding_idx=1)\n",
              "    (rnn): GRU(428, 64)\n",
              "    (output_to_src): Linear(in_features=492, out_features=50004, bias=True)\n",
              "    (output_to_tgt): Linear(in_features=492, out_features=50004, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (criterion): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_batch(data_batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_item, tgt_item in data_batch:\n",
        "        src_batch.append(\n",
        "            torch.cat(\n",
        "                [torch.tensor([BOS_IDX]), src_item, torch.tensor([EOS_IDX])], dim=0\n",
        "            )\n",
        "        )\n",
        "        tgt_batch.append(\n",
        "            torch.cat(\n",
        "                [torch.tensor([BOS_IDX]), tgt_item, torch.tensor([EOS_IDX])], dim=0\n",
        "            )\n",
        "        )\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dl = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_batch,\n",
        "    num_workers=0,\n",
        ")\n",
        "valid_dl = DataLoader(\n",
        "    val_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch,\n",
        "    num_workers=0,\n",
        ")\n",
        "test_dl = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch,\n",
        "    num_workers=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "  warning_cache.warn(\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | encoder   | Encoder          | 150 K \n",
            "1 | decoder   | Decoder          | 49.4 M\n",
            "2 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "49.6 M    Trainable params\n",
            "0         Non-trainable params\n",
            "49.6 M    Total params\n",
            "198.206   Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking: 0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   4%|‚ñç         | 10/227 [09:46<3:32:04, 58.64s/it, v_num=27, train_loss=18.80]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(\n",
        "    gradient_clip_val=1.0,\n",
        "    max_epochs=10,\n",
        "    callbacks=[plc.TQDMProgressBar(refresh_rate=5)],\n",
        ")\n",
        "trainer.fit(model, train_dataloaders=[train_dl], val_dataloaders=[valid_dl])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "src_in = torch.tensor(\n",
        "    [src_vocab[token] for token in tokenizer(\"<bos> ich liebe kartoffeln <eos>\")]\n",
        ").unsqueeze(1)\n",
        "tgt_in = torch.tensor(\n",
        "    [tgt_vocab[token] for token in tokenizer(\"<bos> i love potatoes <eos>\")]\n",
        ").unsqueeze(\n",
        "    1\n",
        ")  # actually unused\n",
        "ic(src_in.shape)\n",
        "ic(src_in)\n",
        "ic(tgt_in.shape)\n",
        "ic(tgt_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ic(torch.tensor(tuple(tgt_vocab[\"<bos>\"] for _ in src_in[:, 0])).unsqueeze(-1).shape)\n",
        "ic(torch.tensor(tuple(tgt_vocab[\"<bos>\"] for _ in src_in[:, 0])).unsqueeze(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tgt_in.shape\n",
        "ic(tgt_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output = model(src_in, tgt_in, Language.src, Language.tgt, teacher_forcing_ratio=0)\n",
        "torch.set_printoptions(profile=\"full\")\n",
        "predicted_tokens = output.argmax(-1)\n",
        "ic(output.shape)\n",
        "ic(predicted_tokens.shape)\n",
        "ic(predicted_tokens)\n",
        "ic([tgt_vocab.get_itos()[t] for t in predicted_tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output = model(\n",
        "    src_in,\n",
        "    torch.tensor(tuple(tgt_vocab[\"<bos>\"] for _ in src_in[:, 0])).unsqueeze(-1),\n",
        "    Language.src,\n",
        "    Language.tgt,\n",
        "    teacher_forcing_ratio=0,\n",
        ")\n",
        "torch.set_printoptions(profile=\"full\")\n",
        "predicted_tokens = output.argmax(-1)\n",
        "ic(output.shape)\n",
        "ic(predicted_tokens.shape)\n",
        "ic(predicted_tokens)\n",
        "ic([tgt_vocab.get_itos()[t] for t in predicted_tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for src_in, tgt_in in valid_dl:\n",
        "    ic(src_in.shape)\n",
        "    ic(tgt_in.shape)\n",
        "    src_in = src_in[:, 0].unsqueeze(-1)  # first item in the batch only\n",
        "    tgt_in = tgt_in[:, 0].unsqueeze(-1)  # first item in the batch only\n",
        "    ic(src_in.shape)\n",
        "    ic(tgt_in.shape)\n",
        "    ic(src_in[:, 0])\n",
        "    ic(tgt_in[:, 0])\n",
        "    break\n",
        "output = model(src_in, tgt_in,  Language.src, Language.tgt, teacher_forcing_ratio=0)\n",
        "torch.set_printoptions(profile=\"full\")\n",
        "predicted_tokens = output.argmax(-1)\n",
        "ic(output.shape)\n",
        "ic(predicted_tokens.shape)\n",
        "ic(predicted_tokens[:, 0])  # get first batch here\n",
        "ic(\n",
        "    [src_vocab.get_itos()[t] for t in src_in[:12]]\n",
        ")  # limit tokens to first 12 for better presentation\n",
        "ic(\n",
        "    [tgt_vocab.get_itos()[t] for t in predicted_tokens[:12]]\n",
        ")  # limit tokens to first 12 for better presentation"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "0nmt-W1lV1jMD-py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
