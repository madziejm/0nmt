{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path '0nmt' already exists and is not an empty directory.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ignoring colorama: markers 'python_version >= \"3.10\" and python_version < \"4.0\" and platform_system == \"Windows\" or python_version >= \"3.10\" and python_version < \"4.0\" and sys_platform == \"win32\"' don't match your environment\n",
            "Requirement already satisfied: aiohttp==3.8.4 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 1)) (3.8.4)\n",
            "Requirement already satisfied: aiosignal==1.3.1 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 89)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout==4.0.2 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 92)) (4.0.2)\n",
            "Requirement already satisfied: attrs==23.1.0 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 95)) (23.1.0)\n",
            "Requirement already satisfied: blis==0.7.9 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 98)) (0.7.9)\n",
            "Requirement already satisfied: catalogue==2.0.8 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 127)) (2.0.8)\n",
            "Requirement already satisfied: certifi==2023.5.7 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 130)) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer==3.1.0 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 133)) (3.1.0)\n",
            "Requirement already satisfied: click==8.1.3 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 209)) (8.1.3)\n",
            "Requirement already satisfied: confection==0.0.4 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 215)) (0.0.4)\n",
            "Requirement already satisfied: contourpy==1.0.7 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 218)) (1.0.7)\n",
            "Requirement already satisfied: cycler==0.11.0 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 274)) (0.11.0)\n",
            "Requirement already satisfied: cymem==2.0.7 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 277)) (2.0.7)\n",
            "Requirement already satisfied: filelock==3.12.0 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 306)) (3.12.0)\n",
            "Requirement already satisfied: fonttools==4.39.4 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 309)) (4.39.4)\n",
            "Requirement already satisfied: frozenlist==1.3.3 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 312)) (1.3.3)\n",
            "Requirement already satisfied: fsspec[http]==2023.5.0 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 387)) (2023.5.0)\n",
            "Requirement already satisfied: idna==3.4 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 390)) (3.4)\n",
            "Requirement already satisfied: jinja2==3.1.2 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 393)) (3.1.2)\n",
            "Requirement already satisfied: kiwisolver==1.4.4 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 396)) (1.4.4)\n",
            "Requirement already satisfied: langcodes==3.3.0 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 465)) (3.3.0)\n",
            "Requirement already satisfied: lightning-utilities==0.8.0 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 468)) (0.8.0)\n",
            "Requirement already satisfied: markupsafe==2.1.3 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 471)) (2.1.3)\n",
            "Requirement already satisfied: matplotlib==3.7.1 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 522)) (3.7.1)\n",
            "Requirement already satisfied: mpmath==1.3.0 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 564)) (1.3.0)\n",
            "Requirement already satisfied: multidict==6.0.4 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 567)) (6.0.4)\n",
            "Requirement already satisfied: murmurhash==1.0.9 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 642)) (1.0.9)\n",
            "Requirement already satisfied: networkx==3.1 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 671)) (3.1)\n",
            "Requirement already satisfied: numpy==1.24.3 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 674)) (1.24.3)\n",
            "Requirement already satisfied: packaging==23.1 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 703)) (23.1)\n",
            "Requirement already satisfied: pathy==0.10.1 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 706)) (0.10.1)\n",
            "Requirement already satisfied: pillow==9.5.0 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 709)) (9.5.0)\n",
            "Requirement already satisfied: preshed==3.0.8 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 776)) (3.0.8)\n",
            "Requirement already satisfied: pydantic==1.10.8 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 805)) (1.10.8)\n",
            "Requirement already satisfied: pyparsing==3.0.9 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 842)) (3.0.9)\n",
            "Requirement already satisfied: python-dateutil==2.8.2 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 845)) (2.8.2)\n",
            "Requirement already satisfied: pytorch-lightning==2.0.2 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 848)) (2.0.2)\n",
            "Requirement already satisfied: pyyaml==6.0 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 851)) (6.0)\n",
            "Requirement already satisfied: requests==2.31.0 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 892)) (2.31.0)\n",
            "Requirement already satisfied: setuptools==67.8.0 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 895)) (67.8.0)\n",
            "Requirement already satisfied: six==1.16.0 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 898)) (1.16.0)\n",
            "Requirement already satisfied: smart-open==6.3.0 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 901)) (6.3.0)\n",
            "Requirement already satisfied: spacy-legacy==3.0.12 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 904)) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers==1.0.4 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 907)) (1.0.4)\n",
            "Requirement already satisfied: spacy==3.5.3 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 910)) (3.5.3)\n",
            "Requirement already satisfied: srsly==2.4.6 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 939)) (2.4.6)\n",
            "Requirement already satisfied: sympy==1.12 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 968)) (1.12)\n",
            "Requirement already satisfied: thinc==8.1.10 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 971)) (8.1.10)\n",
            "Requirement already satisfied: torch==2.0.1 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 1000)) (2.0.1)\n",
            "Requirement already satisfied: torchdata==0.6.1 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 1021)) (0.6.1)\n",
            "Requirement already satisfied: torchmetrics==0.11.4 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 1039)) (0.11.4)\n",
            "Requirement already satisfied: torchtext==0.15.2 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 1042)) (0.15.2)\n",
            "Requirement already satisfied: tqdm==4.65.0 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 1063)) (4.65.0)\n",
            "Requirement already satisfied: typer==0.7.0 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 1066)) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions==4.6.3 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 1069)) (4.6.3)\n",
            "Requirement already satisfied: urllib3==2.0.2 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 1072)) (2.0.2)\n",
            "Requirement already satisfied: wasabi==1.1.2 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 1075)) (1.1.2)\n",
            "Requirement already satisfied: yarl==1.9.2 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from -r ./0nmt/requirements.txt (line 1078)) (1.9.2)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from torch==2.0.1->-r ./0nmt/requirements.txt (line 1000)) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from torch==2.0.1->-r ./0nmt/requirements.txt (line 1000)) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from torch==2.0.1->-r ./0nmt/requirements.txt (line 1000)) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from torch==2.0.1->-r ./0nmt/requirements.txt (line 1000)) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from torch==2.0.1->-r ./0nmt/requirements.txt (line 1000)) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from torch==2.0.1->-r ./0nmt/requirements.txt (line 1000)) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from torch==2.0.1->-r ./0nmt/requirements.txt (line 1000)) (11.7.91)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from torch==2.0.1->-r ./0nmt/requirements.txt (line 1000)) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from torch==2.0.1->-r ./0nmt/requirements.txt (line 1000)) (10.2.10.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from torch==2.0.1->-r ./0nmt/requirements.txt (line 1000)) (2.0.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from torch==2.0.1->-r ./0nmt/requirements.txt (line 1000)) (11.7.99)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from torch==2.0.1->-r ./0nmt/requirements.txt (line 1000)) (2.14.3)\n",
            "Requirement already satisfied: wheel in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch==2.0.1->-r ./0nmt/requirements.txt (line 1000)) (0.40.0)\n",
            "Requirement already satisfied: lit in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1->-r ./0nmt/requirements.txt (line 1000)) (16.0.5.post0)\n",
            "Requirement already satisfied: cmake in /home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages (from triton==2.0.0->torch==2.0.1->-r ./0nmt/requirements.txt (line 1000)) (3.26.3)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "No module named 'google.colab'\n"
          ]
        }
      ],
      "source": [
        "# nasty hack for Colab\n",
        "![ -n $COLAB_RELEASE_TAG ] && git clone -b madziejm-dev https://github.com/madziejm/0nmt.git\n",
        "![ -n $COLAB_RELEASE_TAG ] && pip install -r ./0nmt/requirements.txt\n",
        "try:\n",
        "  import google.colab\n",
        "  import sys\n",
        "  sys.path.insert(0, '/content/0nmt')\n",
        "except Exception as e:\n",
        "  print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import pytorch_lightning.callbacks as plc\n",
        "import torch\n",
        "from icecream import ic\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "from torchtext.vocab import FastText, vocab\n",
        "\n",
        "from zeronmt.models.datatypes import DimensionSpec, Language, Vectors\n",
        "from zeronmt.models.seq2seq import Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# url_base = \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/\"\n",
        "# train_urls = (\"train.de.gz\", \"train.en.gz\")\n",
        "# val_urls = (\"val.de.gz\", \"val.en.gz\")\n",
        "# test_urls = (\"test_2016_flickr.de.gz\", \"test_2016_flickr.en.gz\")\n",
        "\n",
        "# train_filepaths = [\n",
        "#     extract_archive(download_from_url(url_base + url))[0] for url in train_urls\n",
        "# ]\n",
        "# val_filepaths = [\n",
        "#     extract_archive(download_from_url(url_base + url))[0] for url in val_urls\n",
        "# ]\n",
        "# test_filepaths = [\n",
        "#     extract_archive(download_from_url(url_base + url))[0] for url in test_urls\n",
        "# ]\n",
        "\n",
        "# tokenizer = get_tokenizer(\"basic_english\")  # keep it simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "wikimatrix_url = \"https://object.pouta.csc.fi/OPUS-WikiMatrix/v1/moses/de-en.txt.zip\"\n",
        "# train_urls = (\"train.de.gz\", \"train.en.gz\")\n",
        "# val_urls = (\"val.de.gz\", \"val.en.gz\")\n",
        "test_urls = tuple(\n",
        "    \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/\"\n",
        "    + filename\n",
        "    for filename in (\"test_2016_flickr.de.gz\", \"test_2016_flickr.en.gz\")\n",
        ")\n",
        "\n",
        "wikimatrix_filepaths = tuple(\n",
        "    fp\n",
        "    for fp in extract_archive(download_from_url(wikimatrix_url))\n",
        "    if any(f in fp for f in (\"WikiMatrix.de-en.de\", \"WikiMatrix.de-en.en\"))\n",
        ")\n",
        "\n",
        "# val_filepaths = [\n",
        "#     extract_archive(download_from_url(url_base + url))[0] for url in val_urls\n",
        "# ]\n",
        "test_filepaths = [extract_archive(download_from_url(url))[0] for url in test_urls]\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")  # keep it simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('/home/maciej/github/0nmt/notebooks/.data/WikiMatrix.de-en.de',\n",
              " '/home/maciej/github/0nmt/notebooks/.data/WikiMatrix.de-en.en')"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wikimatrix_filepaths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MAPPING_PATH = Path(\n",
        "#     \"/home/maciej/github/bachelor-thesis/project/vecs/le0n8xvt7l/best_mapping.pth\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # TODO\n",
        "# mapping = torch.load(MAPPING_PATH)\n",
        "\n",
        "# cs_vecs = MappedFastTextVectors(language=\"cs\", mapping=None)\n",
        "# pl_vecs = MappedFastTextVectors(language=\"pl\", mapping=mapping)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`src` means DE.  \n",
        "`tgt` means ENG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FastTextPretrainedAligned(FastText):\n",
        "    url_base = (\n",
        "        \"https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.{}.align.vec\"\n",
        "    )\n",
        "    # url_base = \"https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.{}.align.vec\"\n",
        "\n",
        "    def __init__(self, language: str, special_toks: List[str], **kwargs) -> None:\n",
        "        super().__init__(language, **kwargs)\n",
        "\n",
        "        # prepend specials tokens\n",
        "        self.itos[0:0] = special_toks\n",
        "\n",
        "        # hopefully it is not slow :)\n",
        "        self.stoi = {\n",
        "            **dict(zip(special_toks, range(len(special_toks)))),\n",
        "            **{word: i + len(special_toks) for i, word in enumerate(self.stoi)},\n",
        "        }\n",
        "\n",
        "        # the vectors for the special tokens here will not be used by the model\n",
        "        # we set them to zeros so indexing works flawlessly\n",
        "        vecs_special_toks = torch.zeros(len(special_toks), self.dim)\n",
        "        self.vectors = torch.cat((vecs_special_toks, self.vectors), dim=0)\n",
        "        assert len(self.vectors) == len(self.itos)\n",
        "        assert len(self.vectors) == len(self.stoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "VOCAB_SIZE = int(5e4)  # top 50K words only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "specials = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
        "\n",
        "tgt_vecs = FastTextPretrainedAligned(\n",
        "    language=\"en\", special_toks=specials, max_vectors=VOCAB_SIZE\n",
        ")\n",
        "src_vecs = FastTextPretrainedAligned(\n",
        "    language=\"de\", special_toks=specials, max_vectors=VOCAB_SIZE\n",
        ")\n",
        "\n",
        "tgt_vocab = vocab(tgt_vecs.stoi, min_freq=0)\n",
        "src_vocab = vocab(src_vecs.stoi, min_freq=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "src_vocab.set_default_index(src_vocab[\"<unk>\"])\n",
        "tgt_vocab.set_default_index(tgt_vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| src_vecs.stoi[\"<unk>\"]: 0\n",
            "ic| src_vecs.stoi[\"<pad>\"]: 1\n",
            "ic| src_vecs.stoi[\"<bos>\"]: 2\n",
            "ic| src_vecs.stoi[\"<eos>\"]: 3\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ic(src_vecs.stoi[\"<unk>\"])\n",
        "ic(src_vecs.stoi[\"<pad>\"])\n",
        "ic(src_vecs.stoi[\"<bos>\"])\n",
        "ic(src_vecs.stoi[\"<eos>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "# special tokens are prepended, so these indices are the same for both the languages\n",
        "PAD_IDX = src_vocab[\"<pad>\"]\n",
        "BOS_IDX = src_vocab[\"<bos>\"]\n",
        "EOS_IDX = src_vocab[\"<eos>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| PAD_IDX: 1\n",
            "ic| BOS_IDX: 2\n",
            "ic| EOS_IDX: 3\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ic(PAD_IDX)\n",
        "ic(BOS_IDX)\n",
        "ic(EOS_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO\n",
        "# INPUT_DIM = len(cs_vecs)\n",
        "# OUTPUT_DIM = len(pl_vecs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "# INPUT_DIM = len(src_vecs)\n",
        "# OUTPUT_DIM = len(tgt_vecs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ic(INPUT_DIM)\n",
        "# ic(OUTPUT_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENC_HID_DIM = 64\n",
        "DEC_HID_DIM = 64\n",
        "ATTN_DIM = 8\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_process(filepaths):\n",
        "    raw_src_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "    raw_tgt_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "    data = []\n",
        "    for raw_de, raw_en in zip(raw_src_iter, raw_tgt_iter):\n",
        "        src_tensor_ = torch.tensor(\n",
        "            [src_vocab[token] for token in tokenizer(raw_de)], dtype=torch.long\n",
        "        )\n",
        "        tgt_tensor_ = torch.tensor(\n",
        "            [tgt_vocab[token] for token in tokenizer(raw_en)], dtype=torch.long\n",
        "        )\n",
        "        data.append((src_tensor_, tgt_tensor_))\n",
        "    return data\n",
        "\n",
        "\n",
        "train_val_data = data_process(wikimatrix_filepaths)\n",
        "# val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embedding_src.weight\n",
            "embedding_tgt.weight\n",
            "encoder.special_toks_embedding.weight\n",
            "encoder.rnn.weight_ih_l0\n",
            "encoder.rnn.weight_hh_l0\n",
            "encoder.rnn.bias_ih_l0\n",
            "encoder.rnn.bias_hh_l0\n",
            "encoder.rnn.weight_ih_l0_reverse\n",
            "encoder.rnn.weight_hh_l0_reverse\n",
            "encoder.rnn.bias_ih_l0_reverse\n",
            "encoder.rnn.bias_hh_l0_reverse\n",
            "encoder.fc.weight\n",
            "encoder.fc.bias\n",
            "decoder.attention.attn.weight\n",
            "decoder.attention.attn.bias\n",
            "decoder.special_toks_embedding.weight\n",
            "decoder.rnn.weight_ih_l0\n",
            "decoder.rnn.weight_hh_l0\n",
            "decoder.rnn.bias_ih_l0\n",
            "decoder.rnn.bias_hh_l0\n",
            "decoder.output_to_src.weight\n",
            "decoder.output_to_src.bias\n",
            "decoder.output_to_tgt.weight\n",
            "decoder.output_to_tgt.bias\n"
          ]
        }
      ],
      "source": [
        "# enc = Encoder(\n",
        "#     INPUT_DIM, tgt_vecs, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT, PAD_IDX, len(specials)\n",
        "# )\n",
        "# attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
        "# dec = Decoder(\n",
        "#     OUTPUT_DIM,\n",
        "#     src_vecs,\n",
        "#     ENC_HID_DIM,\n",
        "#     DEC_HID_DIM,\n",
        "#     DEC_DROPOUT,\n",
        "#     attn,\n",
        "#     PAD_IDX,\n",
        "#     len(specials),\n",
        "# )\n",
        "\n",
        "# model = Seq2Seq(\n",
        "#     INPUT_DIM,\n",
        "#     PAD_IDX=PAD_IDX\n",
        "#     ).to(device)\n",
        "model = Seq2Seq(\n",
        "    DEC_DROPOUT,\n",
        "    ENC_DROPOUT,\n",
        "    DimensionSpec(\n",
        "        attention=ATTN_DIM,\n",
        "        dec_hid=DEC_HID_DIM,\n",
        "        enc_hid=ENC_HID_DIM,\n",
        "        nspecial_toks=len(specials),\n",
        "    ),\n",
        "    PAD_IDX,\n",
        "    Vectors(src_vecs, tgt_vecs),\n",
        ")\n",
        "# src_pretrained_embeddings=src_vecs,\n",
        "# tgt_pretrained_embeddings=tgt_vecs,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (embedding_src): Embedding(50004, 300)\n",
              "  (embedding_tgt): Embedding(50004, 300)\n",
              "  (encoder): Encoder(\n",
              "    (embedding_src): Embedding(50004, 300)\n",
              "    (embedding_tgt): Embedding(50004, 300)\n",
              "    (special_toks_embedding): Embedding(4, 300, padding_idx=1)\n",
              "    (rnn): GRU(300, 64, bidirectional=True)\n",
              "    (fc): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding_src): Embedding(50004, 300)\n",
              "    (embedding_tgt): Embedding(50004, 300)\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=192, out_features=8, bias=True)\n",
              "    )\n",
              "    (special_toks_embedding): Embedding(4, 300, padding_idx=1)\n",
              "    (rnn): GRU(428, 64)\n",
              "    (output_to_src): Linear(in_features=492, out_features=50004, bias=True)\n",
              "    (output_to_tgt): Linear(in_features=492, out_features=50004, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (criterion): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_batch(data_batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_item, tgt_item in data_batch:\n",
        "        src_batch.append(\n",
        "            torch.cat(\n",
        "                [torch.tensor([BOS_IDX]), src_item, torch.tensor([EOS_IDX])], dim=0\n",
        "            )\n",
        "        )\n",
        "        tgt_batch.append(\n",
        "            torch.cat(\n",
        "                [torch.tensor([BOS_IDX]), tgt_item, torch.tensor([EOS_IDX])], dim=0\n",
        "            )\n",
        "        )\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_len = int(len(train_val_data) * 0.7)\n",
        "val_len = len(train_val_data) - train_len\n",
        "train_data, val_data = train_val_data[:train_len], train_val_data[:train_len]\n",
        "del train_val_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1101406"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(train_data)\n",
        "len(val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dl = DataLoader(\n",
        "    train_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    collate_fn=collate_batch,\n",
        "    num_workers=0,\n",
        ")\n",
        "valid_dl = DataLoader(\n",
        "    val_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch,\n",
        "    num_workers=0,\n",
        ")\n",
        "test_dl = DataLoader(\n",
        "    test_data,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    collate_fn=collate_batch,\n",
        "    num_workers=0,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch.nn.modules.sparse.Embedding'> Embedding(50004, 300)\n",
            "<class 'torch.nn.modules.sparse.Embedding'> Embedding(50004, 300)\n",
            "<class 'zeronmt.models.encoder.Encoder'> Encoder(\n",
            "  (embedding_src): Embedding(50004, 300)\n",
            "  (embedding_tgt): Embedding(50004, 300)\n",
            "  (special_toks_embedding): Embedding(4, 300, padding_idx=1)\n",
            "  (rnn): GRU(300, 64, bidirectional=True)\n",
            "  (fc): Linear(in_features=128, out_features=64, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "<class 'zeronmt.models.decoder.Decoder'> Decoder(\n",
            "  (embedding_src): Embedding(50004, 300)\n",
            "  (embedding_tgt): Embedding(50004, 300)\n",
            "  (attention): Attention(\n",
            "    (attn): Linear(in_features=192, out_features=8, bias=True)\n",
            "  )\n",
            "  (special_toks_embedding): Embedding(4, 300, padding_idx=1)\n",
            "  (rnn): GRU(428, 64)\n",
            "  (output_to_src): Linear(in_features=492, out_features=50004, bias=True)\n",
            "  (output_to_tgt): Linear(in_features=492, out_features=50004, bias=True)\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            ")\n",
            "<class 'torch.nn.modules.loss.CrossEntropyLoss'> CrossEntropyLoss()\n"
          ]
        }
      ],
      "source": [
        "for buf in model.buffers():\n",
        "    print(type(buf), buf.size())\n",
        "for buf in model.children():\n",
        "    print(type(buf), buf)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m81bcd48a2\u001b[0m (\u001b[33m0nmt\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>./wandb/run-20230615_170809-4fi2zqat</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/0nmt/0nmt/runs/4fi2zqat' target=\"_blank\">youthful-dragon-6</a></strong> to <a href='https://wandb.ai/0nmt/0nmt' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/0nmt/0nmt' target=\"_blank\">https://wandb.ai/0nmt/0nmt</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/0nmt/0nmt/runs/4fi2zqat' target=\"_blank\">https://wandb.ai/0nmt/0nmt/runs/4fi2zqat</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from pytorch_lightning.loggers import WandbLogger\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\")\n",
        "wandb_logger = WandbLogger(project=\"0nmt\", log_model=\"all\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name          | Type             | Params\n",
            "---------------------------------------------------\n",
            "0 | embedding_src | Embedding        | 15.0 M\n",
            "1 | embedding_tgt | Embedding        | 15.0 M\n",
            "2 | encoder       | Encoder          | 30.2 M\n",
            "3 | decoder       | Decoder          | 79.4 M\n",
            "4 | criterion     | CrossEntropyLoss | 0     \n",
            "---------------------------------------------------\n",
            "49.6 M    Trainable params\n",
            "30.0 M    Non-trainable params\n",
            "79.6 M    Total params\n",
            "318.216   Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking: 0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0:   0%|          | 0/8605 [00:00<?, ?it/s] "
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(\n",
        "    gradient_clip_val=1.0,\n",
        "    max_epochs=50,\n",
        "    callbacks=[plc.TQDMProgressBar(refresh_rate=3), checkpoint_callback],\n",
        "    logger=wandb_logger\n",
        ")\n",
        "trainer.fit(model, train_dataloaders=[train_dl], val_dataloaders=[valid_dl], )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| src_in.shape: torch.Size([5, 1])\n",
            "ic| src_in: tensor([[    2],\n",
            "                    [   47],\n",
            "                    [  969],\n",
            "                    [15732],\n",
            "                    [    3]])\n",
            "ic| tgt_in.shape: torch.Size([5, 1])\n",
            "ic| tgt_in: tensor([[    2],\n",
            "                    [   32],\n",
            "                    [  571],\n",
            "                    [14391],\n",
            "                    [    3]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[    2],\n",
              "        [   32],\n",
              "        [  571],\n",
              "        [14391],\n",
              "        [    3]])"
            ]
          },
          "execution_count": 109,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "src_in = torch.tensor(\n",
        "    [src_vocab[token] for token in tokenizer(\"<bos> ich liebe kartoffeln <eos>\")]\n",
        ").unsqueeze(1)\n",
        "tgt_in = torch.tensor(\n",
        "    [tgt_vocab[token] for token in tokenizer(\"<bos> i love potatoes <eos>\")]\n",
        ").unsqueeze(\n",
        "    1\n",
        ")  # actually unused\n",
        "ic(src_in.shape)\n",
        "ic(src_in)\n",
        "ic(tgt_in.shape)\n",
        "ic(tgt_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| torch.tensor(tuple(tgt_vocab[\"<bos>\"] for _ in src_in[:, 0])).unsqueeze(-1).shape: torch.Size([5, 1])\n",
            "ic| torch.tensor(tuple(tgt_vocab[\"<bos>\"] for _ in src_in[:, 0])).unsqueeze(-1): tensor([[2],\n",
            "                                                                                         [2],\n",
            "                                                                                         [2],\n",
            "                                                                                         [2],\n",
            "                                                                                         [2]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[2],\n",
              "        [2],\n",
              "        [2],\n",
              "        [2],\n",
              "        [2]])"
            ]
          },
          "execution_count": 110,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ic(torch.tensor(tuple(tgt_vocab[\"<bos>\"] for _ in src_in[:, 0])).unsqueeze(-1).shape)\n",
        "ic(torch.tensor(tuple(tgt_vocab[\"<bos>\"] for _ in src_in[:, 0])).unsqueeze(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| tgt_in: tensor([[    2],\n",
            "                    [   32],\n",
            "                    [  571],\n",
            "                    [14391],\n",
            "                    [    3]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[    2],\n",
              "        [   32],\n",
              "        [  571],\n",
              "        [14391],\n",
              "        [    3]])"
            ]
          },
          "execution_count": 111,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tgt_in.shape\n",
        "ic(tgt_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| output.shape: torch.Size([5, 1, 50004])\n",
            "ic| predicted_tokens.shape: torch.Size([5, 1])\n",
            "ic| predicted_tokens: tensor([[   0],\n",
            "                              [  25],\n",
            "                              [6329],\n",
            "                              [6329],\n",
            "                              [6329]])\n",
            "ic| [tgt_vocab.get_itos()[t] for t in predicted_tokens]: ['<unk>', 'it', 'su', 'su', 'su']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<unk>', 'it', 'su', 'su', 'su']"
            ]
          },
          "execution_count": 112,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = model(src_in, tgt_in, Language.src, Language.tgt, teacher_forcing_ratio=0)\n",
        "torch.set_printoptions(profile=\"full\")\n",
        "predicted_tokens = output.argmax(-1)\n",
        "ic(output.shape)\n",
        "ic(predicted_tokens.shape)\n",
        "ic(predicted_tokens)\n",
        "ic([tgt_vocab.get_itos()[t] for t in predicted_tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| output.shape: torch.Size([5, 1, 50004])\n",
            "ic| predicted_tokens.shape: torch.Size([5, 1])\n",
            "ic| predicted_tokens: tensor([[    0],\n",
            "                              [   25],\n",
            "                              [28431],\n",
            "                              [28431],\n",
            "                              [28431]])\n",
            "ic| [tgt_vocab.get_itos()[t] for t in predicted_tokens]: ['<unk>', 'it', 'agonist', 'agonist', 'agonist']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<unk>', 'it', 'agonist', 'agonist', 'agonist']"
            ]
          },
          "execution_count": 113,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = model(\n",
        "    src_in,\n",
        "    torch.tensor(tuple(tgt_vocab[\"<bos>\"] for _ in src_in[:, 0])).unsqueeze(-1),\n",
        "    Language.src,\n",
        "    Language.tgt,\n",
        "    teacher_forcing_ratio=0,\n",
        ")\n",
        "torch.set_printoptions(profile=\"full\")\n",
        "predicted_tokens = output.argmax(-1)\n",
        "ic(output.shape)\n",
        "ic(predicted_tokens.shape)\n",
        "ic(predicted_tokens)\n",
        "ic([tgt_vocab.get_itos()[t] for t in predicted_tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| src_in.shape: torch.Size([23, 128])\n",
            "ic| tgt_in.shape: torch.Size([25, 128])\n",
            "ic| src_in.shape: torch.Size([23, 1])\n",
            "ic| tgt_in.shape: torch.Size([25, 1])\n",
            "ic| src_in[:, 0]: tensor([   2, 1695,   21,   34,   66,    8,    0,    5,  203,   32,    8,    0,\n",
            "                             4,    3,    1,    1,    1,    1,    1,    1,    1,    1,    1])\n",
            "ic| tgt_in[:, 0]: tensor([    2,    11,    29,    17,     6, 25597,     4,     6, 27690,     5,\n",
            "                              3,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "                              1,     1,     1,     1,     1])\n",
            "ic| output.shape: torch.Size([25, 1, 50004])\n",
            "ic| predicted_tokens.shape: torch.Size([25, 1])\n",
            "ic| predicted_tokens[:, 0]: tensor([ 0, 25, 35, 25,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
            "                                     1,  1,  1,  1,  1,  1,  1])\n",
            "ic| [src_vocab.get_itos()[t] for t in src_in[:12]]: ['<bos>',\n",
            "                                                     'gott',\n",
            "                                                     'ist',\n",
            "                                                     'nicht',\n",
            "                                                     'nur',\n",
            "                                                     'der',\n",
            "                                                     '<unk>',\n",
            "                                                     ',',\n",
            "                                                     'sondern',\n",
            "                                                     'auch',\n",
            "                                                     'der',\n",
            "                                                     '<unk>']\n",
            "ic| [tgt_vocab.get_itos()[t] for t in predicted_tokens[:12]]: ['<unk>',\n",
            "                                                               'it',\n",
            "                                                               'his',\n",
            "                                                               'it',\n",
            "                                                               '<pad>',\n",
            "                                                               '<pad>',\n",
            "                                                               '<pad>',\n",
            "                                                               '<pad>',\n",
            "                                                               '<pad>',\n",
            "                                                               '<pad>',\n",
            "                                                               '<pad>',\n",
            "                                                               '<pad>']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<unk>',\n",
              " 'it',\n",
              " 'his',\n",
              " 'it',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>',\n",
              " '<pad>']"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for src_in, tgt_in in valid_dl:\n",
        "    ic(src_in.shape)\n",
        "    ic(tgt_in.shape)\n",
        "    src_in = src_in[:, 0].unsqueeze(-1)  # first item in the batch only\n",
        "    tgt_in = tgt_in[:, 0].unsqueeze(-1)  # first item in the batch only\n",
        "    ic(src_in.shape)\n",
        "    ic(tgt_in.shape)\n",
        "    ic(src_in[:, 0])\n",
        "    ic(tgt_in[:, 0])\n",
        "    break\n",
        "output = model(src_in, tgt_in,  Language.src, Language.tgt, teacher_forcing_ratio=0)\n",
        "torch.set_printoptions(profile=\"full\")\n",
        "predicted_tokens = output.argmax(-1)\n",
        "ic(output.shape)\n",
        "ic(predicted_tokens.shape)\n",
        "ic(predicted_tokens[:, 0])  # get first batch here\n",
        "ic(\n",
        "    [src_vocab.get_itos()[t] for t in src_in[:12]]\n",
        ")  # limit tokens to first 12 for better presentation\n",
        "ic(\n",
        "    [tgt_vocab.get_itos()[t] for t in predicted_tokens[:12]]\n",
        ")  # limit tokens to first 12 for better presentation"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "0nmt-W1lV1jMD-py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
