{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "from typing import List\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "import pytorch_lightning.callbacks as plc\n",
        "import torch\n",
        "from icecream import ic\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from torchtext.utils import download_from_url, extract_archive\n",
        "from torchtext.vocab import FastText, vocab\n",
        "\n",
        "from zeronmt.models.attention import Attention\n",
        "from zeronmt.models.decoder import Decoder\n",
        "from zeronmt.models.encoder import Encoder\n",
        "from zeronmt.models.seq2seq import Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "url_base = \"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/\"\n",
        "train_urls = (\"train.de.gz\", \"train.en.gz\")\n",
        "val_urls = (\"val.de.gz\", \"val.en.gz\")\n",
        "test_urls = (\"test_2016_flickr.de.gz\", \"test_2016_flickr.en.gz\")\n",
        "\n",
        "train_filepaths = [\n",
        "    extract_archive(download_from_url(url_base + url))[0] for url in train_urls\n",
        "]\n",
        "val_filepaths = [\n",
        "    extract_archive(download_from_url(url_base + url))[0] for url in val_urls\n",
        "]\n",
        "test_filepaths = [\n",
        "    extract_archive(download_from_url(url_base + url))[0] for url in test_urls\n",
        "]\n",
        "\n",
        "de_tokenizer = get_tokenizer(\"basic_english\")  # keep it simple\n",
        "en_tokenizer = get_tokenizer(\"basic_english\")  # keep it simple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MAPPING_PATH = Path(\n",
        "#     \"/home/maciej/github/bachelor-thesis/project/vecs/le0n8xvt7l/best_mapping.pth\"\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# # TODO\n",
        "# mapping = torch.load(MAPPING_PATH)\n",
        "\n",
        "# cs_vecs = MappedFastTextVectors(language=\"cs\", mapping=None)\n",
        "# pl_vecs = MappedFastTextVectors(language=\"pl\", mapping=mapping)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FastTextPretrainedAligned(FastText):\n",
        "    url_base = (\n",
        "        \"https://dl.fbaipublicfiles.com/fasttext/vectors-aligned/wiki.{}.align.vec\"\n",
        "    )\n",
        "    # url_base = \"https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.{}.align.vec\"\n",
        "\n",
        "    def __init__(self, language: str, special_toks: List[str], **kwargs) -> None:\n",
        "        super().__init__(language, **kwargs)\n",
        "\n",
        "        # prepend specials tokens\n",
        "        self.itos[0:0] = special_toks\n",
        "\n",
        "        # hopefully it is not slow :)\n",
        "        self.stoi = {\n",
        "            **dict(zip(special_toks, range(len(special_toks)))),\n",
        "            **{word: i + len(special_toks) for i, word in enumerate(self.stoi)},\n",
        "        }\n",
        "\n",
        "        # the vectors for the special tokens here will not be used by the model\n",
        "        # we set them to zeros so indexing works flawlessly\n",
        "        vecs_special_toks = torch.zeros(len(special_toks), self.dim)\n",
        "        self.vectors = torch.cat((vecs_special_toks, self.vectors), dim=0)\n",
        "        assert len(self.vectors) == len(self.itos)\n",
        "        assert len(self.vectors) == len(self.stoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "VOCAB_SIZE = int(1.5e5)  # top 15K words only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "specials = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
        "\n",
        "en_vecs = FastTextPretrainedAligned(\n",
        "    language=\"en\", special_toks=specials, max_vectors=VOCAB_SIZE\n",
        ")\n",
        "de_vecs = FastTextPretrainedAligned(\n",
        "    language=\"de\", special_toks=specials, max_vectors=VOCAB_SIZE\n",
        ")\n",
        "\n",
        "en_vocab = vocab(en_vecs.stoi, min_freq=0)\n",
        "de_vocab = vocab(de_vecs.stoi, min_freq=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "de_vocab.set_default_index(de_vocab[\"<unk>\"])\n",
        "en_vocab.set_default_index(en_vocab[\"<unk>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| de_vecs.stoi['<unk>']: 0\n",
            "ic| de_vecs.stoi['<pad>']: 1\n",
            "ic| de_vecs.stoi['<bos>']: 2\n",
            "ic| de_vecs.stoi['<eos>']: 3\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ic(de_vecs.stoi[\"<unk>\"])\n",
        "ic(de_vecs.stoi[\"<pad>\"])\n",
        "ic(de_vecs.stoi[\"<bos>\"])\n",
        "ic(de_vecs.stoi[\"<eos>\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "# special tokens are prepended, so these indices are the same for both the languages\n",
        "PAD_IDX = de_vocab[\"<pad>\"]\n",
        "BOS_IDX = de_vocab[\"<bos>\"]\n",
        "EOS_IDX = de_vocab[\"<eos>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| PAD_IDX: 1\n",
            "ic| BOS_IDX: 2\n",
            "ic| EOS_IDX: 3\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ic(PAD_IDX)\n",
        "ic(BOS_IDX)\n",
        "ic(EOS_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO\n",
        "# INPUT_DIM = len(cs_vecs)\n",
        "# OUTPUT_DIM = len(pl_vecs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "INPUT_DIM = len(de_vecs)\n",
        "OUTPUT_DIM = len(en_vecs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| INPUT_DIM: 150004\n",
            "ic| OUTPUT_DIM: 150004\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "150004"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ic(INPUT_DIM)\n",
        "ic(OUTPUT_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "ENC_HID_DIM = 64\n",
        "DEC_HID_DIM = 64\n",
        "ATTN_DIM = 8\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "def data_process(filepaths):\n",
        "    raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
        "    raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
        "    data = []\n",
        "    for raw_de, raw_en in zip(raw_de_iter, raw_en_iter):\n",
        "        de_tensor_ = torch.tensor(\n",
        "            [de_vocab[token] for token in de_tokenizer(raw_de)], dtype=torch.long\n",
        "        )\n",
        "        en_tensor_ = torch.tensor(\n",
        "            [en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long\n",
        "        )\n",
        "        data.append((de_tensor_, en_tensor_))\n",
        "    return data\n",
        "\n",
        "\n",
        "train_data = data_process(train_filepaths)\n",
        "val_data = data_process(val_filepaths)\n",
        "test_data = data_process(test_filepaths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoder.special_toks_embedding.weight\n",
            "encoder.pretrained_embedding.weight\n",
            "encoder.rnn.weight_ih_l0\n",
            "encoder.rnn.weight_hh_l0\n",
            "encoder.rnn.bias_ih_l0\n",
            "encoder.rnn.bias_hh_l0\n",
            "encoder.rnn.weight_ih_l0_reverse\n",
            "encoder.rnn.weight_hh_l0_reverse\n",
            "encoder.rnn.bias_ih_l0_reverse\n",
            "encoder.rnn.bias_hh_l0_reverse\n",
            "encoder.fc.weight\n",
            "encoder.fc.bias\n",
            "decoder.attention.attn.weight\n",
            "decoder.attention.attn.bias\n",
            "decoder.special_toks_embedding.weight\n",
            "decoder.pretrained_embedding.weight\n",
            "decoder.rnn.weight_ih_l0\n",
            "decoder.rnn.weight_hh_l0\n",
            "decoder.rnn.bias_ih_l0\n",
            "decoder.rnn.bias_hh_l0\n",
            "decoder.out.weight\n",
            "decoder.out.bias\n"
          ]
        }
      ],
      "source": [
        "enc = Encoder(\n",
        "    INPUT_DIM, en_vecs, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT, PAD_IDX, len(specials)\n",
        ")\n",
        "attn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\n",
        "dec = Decoder(\n",
        "    OUTPUT_DIM,\n",
        "    de_vecs,\n",
        "    ENC_HID_DIM,\n",
        "    DEC_HID_DIM,\n",
        "    DEC_DROPOUT,\n",
        "    attn,\n",
        "    PAD_IDX,\n",
        "    len(specials),\n",
        ")\n",
        "model = Seq2Seq(enc, dec, PAD_IDX=PAD_IDX).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (special_toks_embedding): Embedding(4, 300, padding_idx=1)\n",
              "    (pretrained_embedding): Embedding(150004, 300)\n",
              "    (rnn): GRU(300, 64, bidirectional=True)\n",
              "    (fc): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (attention): Attention(\n",
              "      (attn): Linear(in_features=192, out_features=8, bias=True)\n",
              "    )\n",
              "    (special_toks_embedding): Embedding(4, 300, padding_idx=1)\n",
              "    (pretrained_embedding): Embedding(150004, 300)\n",
              "    (rnn): GRU(428, 64)\n",
              "    (out): Linear(in_features=492, out_features=150004, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (criterion): CrossEntropyLoss()\n",
              ")"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collate_batch(data_batch):\n",
        "    de_batch, en_batch = [], []\n",
        "    for de_item, en_item in data_batch:\n",
        "        de_batch.append(\n",
        "            torch.cat(\n",
        "                [torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0\n",
        "            )\n",
        "        )\n",
        "        en_batch.append(\n",
        "            torch.cat(\n",
        "                [torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0\n",
        "            )\n",
        "        )\n",
        "    de_batch = pad_sequence(de_batch, padding_value=PAD_IDX)\n",
        "    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n",
        "    return de_batch, en_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dl = DataLoader(\n",
        "    train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch\n",
        ")\n",
        "valid_dl = DataLoader(\n",
        "    val_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch\n",
        ")\n",
        "test_dl = DataLoader(\n",
        "    test_data, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: False, used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "HPU available: False, using: 0 HPUs\n",
            "/home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: UserWarning: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
            "  warning_cache.warn(\n",
            "\n",
            "  | Name      | Type             | Params\n",
            "-----------------------------------------------\n",
            "0 | encoder   | Encoder          | 45.2 M\n",
            "1 | decoder   | Decoder          | 119 M \n",
            "2 | criterion | CrossEntropyLoss | 0     \n",
            "-----------------------------------------------\n",
            "74.2 M    Trainable params\n",
            "90.0 M    Non-trainable params\n",
            "164 M     Total params\n",
            "656.808   Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking: 0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, val_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 35. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  warning_cache.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                                           "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 30. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  warning_cache.warn(\n",
            "/home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:430: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 6 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
            "  rank_zero_warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 227/227 [4:06:44<00:00, 65.22s/it, v_num=14, train_loss=5.190]  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 23. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  warning_cache.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 29. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  warning_cache.warn(\n",
            "/home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 28. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  warning_cache.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: 100%|██████████| 227/227 [4:08:30<00:00, 65.68s/it, v_num=14, train_loss=5.260]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages/pytorch_lightning/utilities/data.py:76: UserWarning: Trying to infer the `batch_size` from an ambiguous collection. The batch size we found is 33. To avoid any miscalculations, use `self.log(..., batch_size=batch_size)`.\n",
            "  warning_cache.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3:  44%|████▍     | 100/227 [1:47:20<2:16:19, 64.40s/it, v_num=14, train_loss=4.520]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/maciej/.cache/pypoetry/virtualenvs/0nmt-W1lV1jMD-py3.10/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        }
      ],
      "source": [
        "trainer = pl.Trainer(\n",
        "    gradient_clip_val=1.0,\n",
        "    max_epochs=10,\n",
        "    callbacks=[plc.TQDMProgressBar(refresh_rate=5)],\n",
        ")\n",
        "trainer.fit(model, train_dataloaders=[train_dl], val_dataloaders=[valid_dl])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| de_in.shape: torch.Size([5, 1])\n",
            "ic| de_in: tensor([[    2],\n",
            "                   [   47],\n",
            "                   [  969],\n",
            "                   [15732],\n",
            "                   [    3]])\n",
            "ic| en_in.shape: torch.Size([5, 1])\n",
            "ic| en_in: tensor([[    2],\n",
            "                   [   32],\n",
            "                   [  571],\n",
            "                   [14391],\n",
            "                   [    3]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[    2],\n",
              "        [   32],\n",
              "        [  571],\n",
              "        [14391],\n",
              "        [    3]])"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "de_in = torch.tensor(\n",
        "    [de_vocab[token] for token in de_tokenizer(\"<bos> ich liebe kartoffeln <eos>\")]\n",
        ").unsqueeze(1)\n",
        "en_in = torch.tensor(\n",
        "    [en_vocab[token] for token in en_tokenizer(\"<bos> i love potatoes <eos>\")]\n",
        ").unsqueeze(\n",
        "    1\n",
        ")  # actually unused\n",
        "ic(de_in.shape)\n",
        "ic(de_in)\n",
        "ic(en_in.shape)\n",
        "ic(en_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en_vocab[\"<bos>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| torch.tensor(tuple(en_vocab['<bos>'] for _ in de_in[:, 0])).unsqueeze(-1).shape: torch.Size([5, 1])\n",
            "ic| torch.tensor(tuple(en_vocab['<bos>'] for _ in de_in[:, 0])).unsqueeze(-1): tensor([[2],\n",
            "                                                                                       [2],\n",
            "                                                                                       [2],\n",
            "                                                                                       [2],\n",
            "                                                                                       [2]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[2],\n",
              "        [2],\n",
              "        [2],\n",
              "        [2],\n",
              "        [2]])"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ic(torch.tensor(tuple(en_vocab[\"<bos>\"] for _ in de_in[:, 0])).unsqueeze(-1).shape)\n",
        "ic(torch.tensor(tuple(en_vocab[\"<bos>\"] for _ in de_in[:, 0])).unsqueeze(-1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| en_in: tensor([[    2],\n",
            "                   [   32],\n",
            "                   [  571],\n",
            "                   [14391],\n",
            "                   [    3]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[    2],\n",
              "        [   32],\n",
              "        [  571],\n",
              "        [14391],\n",
              "        [    3]])"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en_in.shape\n",
        "ic(en_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| output.shape: torch.Size([5, 1, 150004])\n",
            "ic| predicted_tokens.shape: torch.Size([5, 1])\n",
            "ic| predicted_tokens: tensor([[  0],\n",
            "                              [ 73],\n",
            "                              [383],\n",
            "                              [ 38],\n",
            "                              [ 10]])\n",
            "ic| [en_vocab.get_itos()[t] for t in predicted_tokens]: ['<unk>', 'two', 'men', 'are', 'in']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<unk>', 'two', 'men', 'are', 'in']"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = model(de_in, en_in, teacher_forcing_ratio=0)\n",
        "torch.set_printoptions(profile=\"full\")\n",
        "predicted_tokens = output.argmax(-1)\n",
        "ic(output.shape)\n",
        "ic(predicted_tokens.shape)\n",
        "ic(predicted_tokens)\n",
        "ic([en_vocab.get_itos()[t] for t in predicted_tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| output.shape: torch.Size([5, 1, 150004])\n",
            "ic| predicted_tokens.shape: torch.Size([5, 1])\n",
            "ic| predicted_tokens: tensor([[   0],\n",
            "                              [  73],\n",
            "                              [ 383],\n",
            "                              [  38],\n",
            "                              [2258]])\n",
            "ic| [en_vocab.get_itos()[t] for t in predicted_tokens]: ['<unk>', 'two', 'men', 'are', 'standing']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<unk>', 'two', 'men', 'are', 'standing']"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "output = model(\n",
        "    de_in,\n",
        "    torch.tensor(tuple(en_vocab[\"<bos>\"] for _ in de_in[:, 0])).unsqueeze(-1),\n",
        "    teacher_forcing_ratio=0,\n",
        ")\n",
        "torch.set_printoptions(profile=\"full\")\n",
        "predicted_tokens = output.argmax(-1)\n",
        "ic(output.shape)\n",
        "ic(predicted_tokens.shape)\n",
        "ic(predicted_tokens)\n",
        "ic([en_vocab.get_itos()[t] for t in predicted_tokens])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'<unk>'"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "en_vocab.get_itos()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.set_printoptions(threshold=100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ic| de_in.shape: torch.Size([35, 128])\n",
            "ic| en_in.shape: torch.Size([30, 128])\n",
            "ic| de_in.shape: torch.Size([35, 1])\n",
            "ic| en_in.shape: torch.Size([30, 1])\n",
            "ic| de_in[:, 0]: tensor([    2,    29,   347,    14,  3996, 14532, 18363,    25,    58, 22585,\n",
            "                             3,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "                             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "                             1,     1,     1,     1,     1])\n",
            "ic| en_in[:, 0]: tensor([   2,   16,  168,    8,  383,   38, 7395, 4830, 2604,   16, 4453,    3,\n",
            "                            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
            "                            1,    1,    1,    1,    1,    1])\n",
            "ic| output.shape: torch.Size([30, 1, 150004])\n",
            "ic| predicted_tokens.shape: torch.Size([30, 1])\n",
            "ic| predicted_tokens[:, 0]: tensor([  0,  16, 168,   8,  16,  16,  16,  16,  16,   5,   3,   5,   3,   5,\n",
            "                                      3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,   3,   5,\n",
            "                                      3,   5])\n",
            "ic| [de_vocab.get_itos()[t] for t in de_in[:12]]: ['<bos>',\n",
            "                                                   'eine',\n",
            "                                                   'gruppe',\n",
            "                                                   'von',\n",
            "                                                   'männern',\n",
            "                                                   'lädt',\n",
            "                                                   'baumwolle',\n",
            "                                                   'auf',\n",
            "                                                   'einen',\n",
            "                                                   'lastwagen',\n",
            "                                                   '<eos>',\n",
            "                                                   '<pad>']\n",
            "ic| [en_vocab.get_itos()[t] for t in predicted_tokens[:12]]: ['<unk>', 'a', 'group', 'of', 'a', 'a', 'a', 'a', 'a', '.', '<eos>', '.']\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['<unk>', 'a', 'group', 'of', 'a', 'a', 'a', 'a', 'a', '.', '<eos>', '.']"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "for de_in, en_in in valid_dl:\n",
        "    ic(de_in.shape)\n",
        "    ic(en_in.shape)\n",
        "    de_in = de_in[:, 0].unsqueeze(-1)  # first item in the batch only\n",
        "    en_in = en_in[:, 0].unsqueeze(-1)  # first item in the batch only\n",
        "    ic(de_in.shape)\n",
        "    ic(en_in.shape)\n",
        "    ic(de_in[:, 0])\n",
        "    ic(en_in[:, 0])\n",
        "    break\n",
        "output = model(de_in, en_in, teacher_forcing_ratio=0)\n",
        "torch.set_printoptions(profile=\"full\")\n",
        "predicted_tokens = output.argmax(-1)\n",
        "ic(output.shape)\n",
        "ic(predicted_tokens.shape)\n",
        "ic(predicted_tokens[:, 0])  # get first batch here\n",
        "ic(\n",
        "    [de_vocab.get_itos()[t] for t in de_in[:12]]\n",
        ")  # limit tokens to first 12 for better presentation\n",
        "ic(\n",
        "    [en_vocab.get_itos()[t] for t in predicted_tokens[:12]]\n",
        ")  # limit tokens to first 12 for better presentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "0nmt-W1lV1jMD-py3.10",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
